\section{Impact of Clock Stability on Duty Cycling, Bandwidth, and Power Consumption}
\label{sec:power}

It is generally assumed that using a more stable clock will improve the duty
cycling capabilities of an embedded system and save bandwidth since less 
frequent resynchronizations are necessary. Dutta
showed in \cite{dutta2007procrastination} that the lower bound of a clock with
a stability of $\pm 50$ppm is a duty cycle of 0.01\% for a scheduled
communication MAC protocol. But, how much additional power and bandwidth could be saved by
employing a more stable clock, considering that a more stable clock consumes
more power? 

\subsection{Clock Stability and Duty Cycling}
Let us compare two systems, $A$ and $B$. Both systems have the same sleep power
consumption $P_S$ and active power consumption $P_A$.  Each node has a local
clock source with stability $s_1$ and $s_2$ (in $Hz/Hz$) that consume power $P_{C1}$ and $P_{C2}$,
respectively. Additionally, we assume that both platforms have the same duty
cycle ratio $DC$ between sleep ($T_S$) and active time ($T_A$). However, in order
to communicate with their peers, both systems include a guard time that is
proportional to their local clock source's stability $T_{Gx} = 2 \cdot T_S \cdot
s_x$, where $s_x$ is the system's local clock stability. This guard time
allows the nodes to compensate for the drift in their clock while asleep by starting the synchronization
process early enough to ensure that both nodes are awake when the active period starts.

Given these definitions, we can calculate the average power consumption of the
system and its clock in the sleep, active and guard periods as follows:
\begin{equation}
	P_x = \frac{T_S \cdot (P_S + P_{Cx}) + (T_{Gx} + T_A) \cdot (P_A + P_{Cx})}{T_S + T_A + T_{Gx}}.
\end{equation}

Where $x$ is either $A$ or $B$. Assume that $s_1 > s_2$ and thus $P_{C1} < P_{C2}$. In order for
system $B$ to be more efficient than system $A$ we have to show that
\begin{equation}
	P_A > P_B,
\end{equation}
and thus
\begin{eqnarray}
	\lefteqn{\frac{T_S \cdot (P_S + P_{C1}) + (T_{G1} + T_A) \cdot (P_A + P_{C1})}{T_S + T_A + T_{G1}}  > } \nonumber \\
	& & \frac{T_S \cdot (P_S + P_{C2}) + (T_{G2} + T_A) \cdot (P_A +
	P_{C2})}{T_S + T_A + T_{G2}}.\label{eq:1}
\end{eqnarray}

We now assume that the clock of system $A$ is a cheap low frequency crystal as
found in many embedded systems. These crystals consume very little power, and thus
$P_{C1} \sim 0$. Using this assumption, Equation \ref{eq:1} can be simplified to
\begin{equation}
%	P_{C2} & < & \frac{T_S P_S + (T_{G1} + T_A) \cdot P_A}{T_S + T_A + T_{G1}}
%	\nonumber \\
%	& & - \frac{T_S P_S + (T_{G2} + T_A) \cdot P_A}{T_S + T_A + T_{G2}} \nonumber \\
P_{C2} < \frac{2\cdot T_s^2\cdot(P_A-P_s)\cdot(s_1-s_2)}{(T_S\cdot(1+2s_1)+T_A)(T_S\cdot(1+2s_2)+T_A)},
	\label{eq:2}
\end{equation}
where we replaced $T_{G1}$ and $T_{G2}$ with its respective definition. We can
now observe that $DC=T_A/(T_A + T_S)$ is the duty cycle of the system, and if
we assume that $s_1, s_2 << 1$, we get
\begin{equation}
	P_{C2} < 2 \cdot [1-DC]^2\cdot (P_A-P_S) \cdot (s_1-s_2).
\end{equation}
Further assuming that $DC << 1$, $P_S \rightarrow 0$, and $s_1 >> s_2$ we find that
\begin{equation}
	P_{C2} < 2 \cdot P_A \cdot s_1.
\end{equation}

This shows that in order for system $B$ to be more power efficient than system
$A$, the clock system used in system $B$ has to use less power than the active
power consumption multiplied by twice the precision of system $A$'s clock
stability. For example, if we assume that $P_A=1W$, $s_1=50 ppm$, and $s_2=1
ppm$ (in order to satisfy $s_1 >> s_2$) then $P_{C2} < 100 \mu W$. We
currently don't know of a technology that can achieve a clock stability of
$1ppm$ with a power budget of less than $100 \mu W$. The closest we could find
on the current market is the MAXIM DS32BC35 \cite{maxim2008ds32b35} which is
an RTC with integrated 32kHz TCXO, and consumes about 600mW for a stability of
$\pm 3.5ppm$. However, we also ignored the fact that time synchronization
protocols, such as FTSP \cite{maroti2004ftsp}, can estimate the current clock
drift very accurately and thus, as long as the temperature doesn't change too
quickly, can improve the duty cycling of an embedded system without using a
higher stability clock source.


\subsection{Bandwidth Benefits}

It is trivial to see that a time synchronization protocol based on a less
temperature stable clock needs to resynchronize more often, than one based on
a more stable clock. But how much is the difference? In the simplest case,
which represents an upper bound, the time synchronization algorithm assumes
the worst case drift $s_{\max}$ and calculates the necessary resynchronization
time interval $T_r$ based on a maximum allowed time synchronization error
$\epsilon$

\begin{equation}
    T_r < \frac{\epsilon}{s_{\max}}.
\end{equation}

This is a very conservative estimate and doesn't include the fact that a time
synchronization protocol can calculate the current clock drift. Additionally,
if we have knowledge of how fast temperature changes in the environment where
the system is located, then a much better estimate of $T_r$ can be found. 

\begin{figure}
    \begin{center}
        \includegraphics[angle=-90,width=0.45\textwidth]{figures/mosscamresync}
        \caption{Upper and lower bounds for the number of resynchronizations
        necessary to achieve a given time synchronization error. The
        temperature data comes from a 3 year dataset.}
        \label{fig:resync}
    \end{center}
\end{figure}

The question arises on what would be the optimal resynchronization interval
while still guaranteeing that the synchronization error is smaller than
$\epsilon$? Assuming the system has access to an oracle that tells the system
the current synchronization error, and that the system does not compensate for
local clock drift, it is clear that a system with a lower drift clock will
resynchronize less often than one with a higher drift. Figure \ref{fig:resync}
illustrates this by using a 3 year temperature dataset collected at the James
Natural Wildlife Reserve \cite{mosscam}. It shows the upper and lower bounds
by calculating how often a system would have to resynchronize experiencing the
temperature changes recorded in the dataset. Looking at one specific example,
we can calculate the bandwidth savings a better clock stability can achieve.
Let's assume that the application needs a synchronization accuracy of
$\epsilon<1ms$. Over the 3 year period, an uncompensated clock would have to
resynchronize at least 900'000 times using the oracle, whereas a compensated
TCXO only 25'000 times. Now, assuming that each resynchronization consists of
2 messages of 16 bytes each results in an average bandwidth of 2.35 bit/s for
the uncompensated clock, and only 0.065 bit/s for a compensated clock.

Even though this shows that a compensated clock can lower the number of
necessary resynchronizations, it does not mean that a time synchronization
system needs to employ a TCXO to achieve this compensation. Maroti et al.
showed in \cite{maroti2004ftsp} that FTSP can estimate the current clock drift
below an accuracy of 0.1ppm. Thus, as long as the environment temperature
doesn't change too often, drift compensation can be achieved in software to a
very high accuracy.

